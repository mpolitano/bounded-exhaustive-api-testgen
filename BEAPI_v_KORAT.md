# Comparison of `BEAPI` against `Korat` in benchmarks from the literature (RQ1 in Section 4.1 of the paper)

## Running a single experiment

To easily run either `BEAPI` or `Korat` in benchmarks from the literature we provide `run-testgen-benchmarks.sh` in the `scripts` folder. First, move to the scripts folder.

```
cd $BE_EXP_SRC/scripts
```

`run-testgen-benchmarks.sh` takes the following arguments:
```
./run-testgen-benchmarks.sh <benchmark> <case study> <technique> <scope>
```
where `<benchmark>` is one of `0_korat`, `1_kiasan`, `2_roops`, `3_fajita` (i.e., the name of the folder of the corresponding benchmark); `<case study>` is one of the case studies for `<benchmark>` (see section [Available case studies](#Available-case-studies));  `<technique>` is either `beapi` or `korat`; and `<scope>` is the maximum number of nodes and the number of integers (from 0 to scope-1) available for generation.

For example, to generate inputs for `BinTree` from the `3_fajita` benchmark using `BEAPI` with a scope of `4` execute:
```
./run-testgen-benchmarks.sh 3_fajita bintree.BinTree beapi 4
```

At the end of its execution the script shows a summary of the results in CSV format:

```
************
Report
Project,Class,Technique,Budget,Time,Structures,Explored
3_fajita,bintree.BinTree,beapi,4,.18,51,210
************
```

where:

- **Project**: the name of the selected benchmark.
- **Class**: the name of the case study.
- **Technique**: the generation technique employed.
- **Budget**: the scope (maximum number of nodes, integers, etc...) used for input generation.
- **Time**: the time spent for input generation in seconds.
- **Structures**: the number of structures (equivalent to the number of tests when `BEAPI` is used in this experiment) generated by the technique.
- **Explored**: the number of test sequences explored by `BEAPI`, or the number of structures explored for `Korat`.

Notice the relationship between the output of the script and the data presented in **Table 1** of **Section 4.1** is mostly straightforward. The correspondence from the fully qualified name of a case study (i.e. `bintree.BinTree`) to its abbreviated name (i.e. `BinTree`) is listed below in section [Available case studies](#Available-case-studies). 

A log of the output of the selected tool is saved to file:

```
$BE_EXP_SRC/scripts/results-begen/<benchmark>/<case study>/<technique>/<scope>/log.txt
```

For the execution above, the log for `BEAPI` is in file:

```
$BE_EXP_SRC/scripts/results-begen/3_fajita/bintree.BinTree/beapi/matching/builders/4/log.txt
```  

The results of the (successive) executions of the script are saved in file: 

```
$BE_EXP_SRC/scripts/results-begen/results_testgen_benchmarks.csv
```

For example, after three executions of the script we get something like this:

```
% cat $BE_EXP_SRC/scripts/results-begen/results_testgen_benchmarks.csv

Project,Class,Technique,Budget,Time,Structures,Explored
3_fajita,bintree.BinTree,beapi,3,.07,15,50
3_fajita,bintree.BinTree,beapi,4,.18,51,210
3_fajita,bintree.BinTree,korat,4,0.289,51,1277
```

**Note**: The precomputed builder methods used to run `BEAPI` in this experiment are stored in plain text in `$BE_EXP_SRC/scripts/config/<benchmark>/builders/<case study>` file. For example:

```
% cat $BE_EXP_SRC/scripts/config/3_fajita/builders/bintree.BinTree

bintree.BinTree.<init>\(\)
bintree.BinTree.add\(int\)
```

**Note**: The ***test suite*** generated by `BEAPI` is stored in the `beapi-tests` folder located inside the specified project. `BEAPI` generates suites in the JUnit testing framework. In the example above, the JUnit tests are stored in `$BE_EXP_SRC/3_fajita/beapi-tests`. For efficiency reasons, we do not serialize the objects generated by `Korat` in this experiment.

## Available case studies

- `0_korat`
  - `DoublyLinkedList`: korat.examples.doublylinkedlist.DoublyLinkedList (`DLList`)
  - `FibonacciHeap`: korat.examples.fibheap.FibonacciHeap (`FibHeap`)
  -	`BinomialHeap`: korat.examples.binheap.BinomialHeap (`BinHeap`)
  - `SearchTree`:korat.examples.searchtree.SearchTree (`BST`)
  - `SinglyLinkedList`: korat.examples.singlylinkedlist.SinglyLinkedList (`SLList`)
  - `RedBlackTree`: korat.examples.redblacktree.RedBlackTree (`RBT`)
  - `SortedList`: korat.examples.sortedlist.SortedList (`SortedList`) 

- `1_kiasan`
  - `BinarySearchTree`: binarysearchtree.BinarySearchTree (`BST`)
  - `DoubleLinkedList`: doublylinkedlist.DoubleLinkedList (`DLL`)
  - `TreeSet`: redblacktree.TreeSet(`RBT`)
  - `DisjSetsFast`: disjointSet.fast.DisjSetsFast (`DisjSetFast`)
  - `StackLi`: stack.list.StackLi (`StackList`)
  - `BinaryHeap`: binaryheap.BinaryHeap (`BHeap`)
  - `TreeMap`: redblacktree.TreeMap (`TreeMap`)
  - `DisjSet`: disjointSet.orig.DisjSets (`DisjSet`) 

- `2_roops`

  - `AvlTree`: avl.AvlTree (`AVL`)
  - `NodeCachingLinkedList`: ncl.NodeCachingLinkedList (`NCL`)
  - `BinTree`: bintree.BinTree (`BinTree`)
  - `LinkedList`: linkedlist.LinkedList (`LList`)
  - `TreeSet`: rbt.TreeSet (`RBT`)
  - `FibHeap`: fibheap.FibHeap (`FibHeap`)
  - `BinomialHeap`: bheap.BinomialHeap (`BinHeap`)

- `3_fajita`
  - `BinTree `: bintree.BinTree (`BinTree`)
  - `AvlTree`: avl.AvlTree (`AVL`)
  - `TreeSet`: rbt.TreeSet (`RBT`)
  - `BinomialHeap`: bheap.BinomialHeap (`BinHeap`)
  - `SinglyLinkedList`: list.SinglyLinkedList (`SLList`) 
  - `DoubleLinkedList`: cdlist.LinkedList (`DLList`)
  - `NodeCachingLinkedList`: cList.NodeCachingLinkedList (`NCL`)

Note: In parentheses are the short names used in **Table 1** of **Section 4.1** of the paper to identify each case study. The results for some case studies are not reported in the table for space reasons; these can be found online [Replication Package](https://sites.google.com/view/bounded-exhaustive-api/home).

## Running all experiments from a single benchmark (slow)

To reproduce all the experiments for a specific benchrmark study with both techniques (`Korat` and `beapi`) and for all scopes within 3 and *maxScope* pick and run one of following commands: 

```
./run-testgen-benchmarks-0_korat.sh <maxScope>
./run-testgen-benchmarks-1_kiasan.sh <maxScope>
./run-testgen-benchmarks-2_roops.sh <maxScope>
./run-testgen-benchmarks-3_fajita.sh <maxScope>
```

The results of executing these scripts are saved in file: 

```
$BE_EXP_SRC/scripts/results-begen/results_testgen_benchmarks.csv
```

Note: Running one of the above scripts might take a day depending on your hardware.

## Running all the experiments (very slow)

To reproduce all the experiments for this research question run:
```
./run-testgen-benchmarks-all.sh <maxScope>
```

The results of executing this script are saved in file: 
```
$BE_EXP_SRC/scripts/results-begen/results_testgen_benchmarks.csv
```

Note: Running this script might take a few days depending on your hardware.

